{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Send Airplanes Through Pipelines\n",
    "### or \"How to bundle repeated actions into a single predictable process\"\n",
    "\n",
    "We will be attempting to predict whether a single flight will be delayed, given various characteristics about that flight. The data is provided by Albert Bifet & Elena Ikonomovska, [Data Expo competition (2009)](http://kt.ijs.si/elena_ikonomovska/data.html). Here is the description Elena Ikonomovska gave from her website:   \n",
    "  \n",
    ">  The dataset consists of a large amount of records, containing flight arrival and departure details for all the commercial flights within the USA, from October 1987 to April 2008. This is a large dataset with nearly 120 million records (11.5 GB memory size). The dataset was cleaned and records were sorted according to the arrival/departure date (year, month, and day) and time of flight. Its final size is around 116 million records and 5.76 GB of memory.\n",
    "  \n",
    "We will be using [OpenML](https://www.openml.org/) to access the data, along with `fetch_openml` from sklearn so that we don't even need to worry about unzipping or finding a folder for the data (it's all handled inside python). (The specifics from OpenML about this dataset can be found [here](https://www.openml.org/d/1169).) First, these are the imports for the entire project along with the code to save the data in RAM and read the description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split      \n",
    "from sklearn.datasets import fetch_openml    \n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Author**: Albert Bifet, Elena Ikonomovska  \n",
      "**Source**: [Data Expo competition](http://kt.ijs.si/elena_ikonomovska/data.html) - 2009  \n",
      "**Please cite**:   \n",
      "\n",
      "Airlines Dataset Inspired in the regression dataset from Elena Ikonomovska. The task is to predict whether a given flight will be delayed, given the information of the scheduled departure.\n",
      "\n",
      "Downloaded from openml.org.\n"
     ]
    }
   ],
   "source": [
    "airlines = sklearn.datasets.fetch_openml('airlines', version='1')    # This is a JSON dictionary\n",
    "print(airlines['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Info!\n",
    "Originally, this dataset wasn't gathered for machine learning, but rather for a \"Data Expo\" Here is the original challenge:  \n",
    "\n",
    ">The aim of the data expo is to provide a graphical summary of important features of the data set. This is intentionally vague in order to allow different entries to focus on different aspects of the data.\n",
    "\n",
    "Check out the resulting posters [here](http://stat-computing.org/dataexpo/2009/posters/).\n",
    "\n",
    "Great! Let's do a bit of prep work, then we'll build the pipeline. The purpose here is to input \"Raw\" data, whatever that means for you, and output an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we save it in a DataFrame, I had to hunt around just a bit to find the column names, but any time you begin investigating a new dataset, there's a good chance you'll have metadata that may or may not be useful to you, with the actual data you want nested inside somewhere. For this dataset, it is very obviuosly kept in the `data` part of the main dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline</th>\n",
       "      <th>Flight</th>\n",
       "      <th>AirportFrom</th>\n",
       "      <th>AirportTo</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Time</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>205.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Airline  Flight  AirportFrom  AirportTo  DayOfWeek  Time  Length\n",
       "0      3.0   269.0          2.0        3.0        2.0  15.0   205.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(airlines['data'], columns=airlines['feature_names'])  # These will be the X inputs\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs we are trying to predict are kept inside a different part of the JSON file, they reside in `target` as binary values: 1 means the flight was delayed, while 0 means \"on time\". We will perform our train_test_split right from the start.  \n",
    "#### Important note: it's always a good idea to split your data BEFORE you do you any heavy modifications to it (like one-hot-encoding or fillna). \n",
    "This is because the action of \"modifying\" the data before a split will inherently pass *some* bit of information into the test data that gets pulled out later, which will bias the results of the model. For example, if we have 15% of our data that is `null` but we chose to fill those nulls with the median value for the column, THEN split the data, if the test data has any `null` values at all, it has inadvertanly gained information about the training data, even though that's never supposed to happen. My first data science instructor really tried to drive home this point, and I'll borrow from him in saying that this is a \"Career Limiting Mistake\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.DataFrame(airlines['target'], columns=['was_late'])  # This is the goal: y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, random_state=14159)  # random_state is only so our numbers match, generally not needed except for educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that last step, it's crucial the rows of `X` line up with the rows of `y`. This data set is pre-cleaned and I am willing to trust it, but often you'll specify directly which columns you want from a unified dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/learn-env/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "one_hot = OneHotEncoder()\n",
    "s1 = one_hot.fit(X_train)\n",
    "s2 = s1.transform(X_train)\n",
    "s3 = s2.toarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pipeline([OneHotEncoder])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
